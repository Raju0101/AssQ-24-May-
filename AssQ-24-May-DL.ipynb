{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16cb7fb5-0242-4c9f-9ea2-d9c3608648f4",
   "metadata": {},
   "source": [
    "## AssQ- 24-May- DL-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ae2d4-c96e-4198-ab7b-c46eac8c1c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create and deploy a ML project by i\\porting load_breast_cancer dataset fro\\ sklearnIload_dataset and \n",
    "apply the followingF\n",
    "\n",
    "[H Create a folder in which you want to create the project, after that use the git init and the necessary \n",
    "co\\\\ands to create the specific @it repositoryH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a76c54-1904-480b-a112-38147f5d999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir breast_cancer_ml_project\n",
    "cd breast_cancer_ml_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e3a583-bd65-4fbe-9605-4f6b993907da",
   "metadata": {},
   "outputs": [],
   "source": [
    "git init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c0f86-ed0a-423b-9991-4ec61b9ff591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3ed81c-40b9-421b-b59d-baab46e5b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "git add main.py\n",
    "git commit -m \"Initial commit: Set up project and applied Logistic Regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c10bfb-051b-4150-8e86-cd207056042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "git remote add origin <repository_url>\n",
    "git branch -M main\n",
    "git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb2541-875b-49a7-b124-bbfad0142683",
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_ml_project/\n",
    "├── main.py\n",
    "└── README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe3fe86-d43a-4320-88cb-64a2e8e27b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d441bdaf-da83-4c65-bcc3-cd918114f3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "2 Create a separate environ\\ent so that you do not \\ess up with your base environ\\entH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05675fd9-314c-4c2c-a916-87073eb0298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create --name ml_project_env python=3.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d96eab-a342-44b0-9ee0-0d89f973ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda activate ml_project_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7127ca-410b-444a-a1aa-647595e53fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "source activate ml_project_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91c7316-5071-4817-a5ca-701f9e5711ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc8dc7-c003-4cef-b7ff-9f16457747b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda deactivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2392c64e-3f4b-4acf-8f92-d6c5483eb87d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a691e4-e5da-4d65-987a-4130735d6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "3 Create the folder structure/directories and files using the python progra\\\\e re=uired for a ML projectI \n",
    "You can refer the following project structureF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d5e5ad-a78c-4e98-bbe3-6825a1059c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_project_structure():\n",
    "    project_dirs = [\n",
    "        \"src\",\n",
    "        \"src/components\",\n",
    "        \"src/components/data_ingestion\",\n",
    "        \"src/components/data_transformation\",\n",
    "        \"src/components/model_trainer\",\n",
    "        \"src/pipelines\",\n",
    "    ]\n",
    "\n",
    "    project_files = [\n",
    "        \"__init__.py\",\n",
    "        \"logger.py\",\n",
    "        \"exception.py\",\n",
    "        \"utils.py\",\n",
    "        \"src/components/__init__.py\",\n",
    "        \"src/components/data_ingestion/__init__.py\",\n",
    "        \"src/components/data_ingestion/data_ingestion.py\",\n",
    "        \"src/components/data_transformation/__init__.py\",\n",
    "        \"src/components/data_transformation/data_transformation.py\",\n",
    "        \"src/components/model_trainer/__init__.py\",\n",
    "        \"src/components/model_trainer/model_trainer.py\",\n",
    "        \"src/pipelines/__init__.py\",\n",
    "        \"src/pipelines/predict_pipeline.py\",\n",
    "        \"src/pipelines/train_pipeline.py\",\n",
    "        \"import_data.py\",\n",
    "        \"setup.py\",\n",
    "        \"notebook.ipynb\",\n",
    "        \"requirements.txt\",\n",
    "    ]\n",
    "\n",
    "    for directory in project_dirs:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    for file in project_files:\n",
    "        with open(file, \"w\") as f:\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_project_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb4a62-d233-4eec-9f1f-9c7552b65388",
   "metadata": {},
   "outputs": [],
   "source": [
    "git init\n",
    "git add .\n",
    "git commit -m \"Initial project structure\"\n",
    "git branch -M main  # Rename the default branch to 'main'\n",
    "git remote add origin <repository_url>  # Replace with your repository URL\n",
    "git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cc0012-9e29-4a87-9ef9-1f498937f1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3fc73-30a2-467b-ba39-1ac919627150",
   "metadata": {},
   "outputs": [],
   "source": [
    "4 Write the progra\\ for setupIpy and the relevant dependencies in re=uire\\entsItxt and generate \n",
    "eggIinfo folderH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81896f4-798d-4b42-a3b8-d846a3922f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name=\"your_project_name\",\n",
    "    version=\"0.1\",\n",
    "    author=\"Your Name\",\n",
    "    author_email=\"your@email.com\",\n",
    "    description=\"Description of your project\",\n",
    "    packages=find_packages(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e1cafa-c74f-4a8f-b7d0-3c2211fbc070",
   "metadata": {},
   "outputs": [],
   "source": [
    "scikit-learn\n",
    "# Add other dependencies here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd8f0b4-dc65-47d5-8e2b-ae8b80ec1567",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install setuptools wheel  # Install necessary tools\n",
    "python setup.py sdist bdist_wheel  # Generate distribution files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412fccf-fed8-485a-b587-584a07c1ae81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18486e1-de2c-42af-a766-0c46cb02c914",
   "metadata": {},
   "outputs": [],
   "source": [
    "5 write the logging function in loggerIpy and exception function in exceptionIpy file to be used for the \n",
    "project to track the progress when the ML project is run and to raise any exception when encounteredI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce070c4-c0a1-4588-8323-8653ce57745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def setup_logger(log_file):\n",
    "    \"\"\"Set up a logging instance and configure log file.\"\"\"\n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger()\n",
    "    return logger\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    logger = setup_logger(\"project.log\")\n",
    "    logger.info(\"Starting the ML project...\")\n",
    "    logger.warning(\"This is a warning message.\")\n",
    "    logger.error(\"An error occurred.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f46e6ab-79d3-49ba-9870-2b5eb880f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectError(Exception):\n",
    "    \"\"\"Custom exception class for the project.\"\"\"\n",
    "    def __init__(self, message):\n",
    "        super().__init__(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0293fdb-ad1a-4332-b05e-35c752e71e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.exception import ProjectError\n",
    "\n",
    "def perform_ml_task():\n",
    "    try:\n",
    "        # ... code that might raise an exception ...\n",
    "        raise ProjectError(\"An error occurred during ML task.\")\n",
    "    except ProjectError as e:\n",
    "        logger.error(str(e))  # Log the exception message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161189f-c81f-4f3a-81ed-b0959f7aae16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1460040-8a99-4703-b8fc-b131602f8aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\u001d",
    "F I'=t4e='ote8ook=-ol\u0017er=create=a=jup*ter='ote8ook=i'si\u0017e=it=a'\u0017=\u0017o=t4e=-ollo#i';=#it4=t4e=\u0017ataset\"\n",
    "7 Explorator*=Data=\u001e",
    "'al*si&\n",
    "7 Feature=E';i'eeri':\n",
    "7 Mo\u0017el=Trai'i':\n",
    "7 Selectio'=o-=8est=mo\u0017el=usi';=metri,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15888ce5-5ba9-430b-bbe2-b18cab3bab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "## Introduction\n",
    "Briefly introduce the purpose of this Jupyter Notebook and the dataset you'll be working with.\n",
    "\n",
    "## Load and Inspect the Dataset\n",
    "- Import necessary libraries and modules.\n",
    "- Load the breast cancer dataset using scikit-learn's `load_breast_cancer`.\n",
    "- Display basic information about the dataset, such as shape, feature names, and target names.\n",
    "\n",
    "## Data Analysis and Visualization\n",
    "- Perform exploratory data analysis to gain insights into the dataset.\n",
    "- Generate descriptive statistics for the dataset using Pandas.\n",
    "- Create visualizations such as histograms, scatter plots, and correlation matrices to understand the data \n",
    "distribution and relationships.\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "## Feature Selection\n",
    "- Explain the importance of feature selection.\n",
    "- Discuss different methods for feature selection, e.g., SelectKBest, recursive feature elimination.\n",
    "- Apply feature selection to the dataset to choose the most relevant features.\n",
    "\n",
    "# Model Training\n",
    "\n",
    "## Introduction to Model Training\n",
    "- Explain the concept of model training and evaluation.\n",
    "\n",
    "## Splitting the Dataset\n",
    "- Split the dataset into training and testing sets using scikit-learn's `train_test_split`.\n",
    "\n",
    "## Model Selection\n",
    "- Introduce the concept of selecting the best machine learning model for the task.\n",
    "- Discuss different classification algorithms that could be suitable for this dataset (e.g., Logistic Regression,\n",
    "                                                                                       Random Forest, Support Vector Machine).\n",
    "\n",
    "## Training and Evaluation\n",
    "- Initialize and train multiple models using the training data.\n",
    "- Evaluate each model's performance using appropriate metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "# Model Deployment and Conclusion\n",
    "\n",
    "## Selecting the Best Model\n",
    "- Analyze the evaluation results to select the best-performing model.\n",
    "\n",
    "## Model Deployment (Optional)\n",
    "- If applicable, briefly explain the concept of model deployment in a real-world scenario.\n",
    "\n",
    "## Conclusion\n",
    "- Summarize the findings from the analysis and model training.\n",
    "- Reflect on the project's goals and outcomes.\n",
    "\n",
    "## Future Work (Optional)\n",
    "- Suggest potential areas for further improvement, such as hyperparameter tuning or using more advanced techniques.\n",
    "\n",
    "# References (Optional)\n",
    "List any sources or references you used during the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5562fdee-3c2d-4701-9867-07e1bb9fb532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587959f6-514b-4a87-8e51-fbfcade25473",
   "metadata": {},
   "outputs": [],
   "source": [
    "\u0016F Write=a=separate=p*t4o'=pro;ram=i'=import_\u0017ata.p*=-ile=to=loa\u0017=t4e=me'tio'e\u0017=\u0017ataset=-rom=\n",
    "sklear'.loa\u0017_\u0017ataset.loa\u0017_8reast_ca'cer=to=*our=Mo';oD F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b60e3c-2696-4256-9409-94cfdbed15ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import_data.py\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"Load and return the breast cancer dataset.\"\"\"\n",
    "    data = load_breast_cancer()\n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = load_dataset()\n",
    "    print(\"Breast Cancer Dataset Loaded:\")\n",
    "    print(\"Features shape:\", dataset.data.shape)\n",
    "    print(\"Target shape:\", dataset.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaccfb8-fd0e-497a-9c3f-16f9a7324ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dcfb01-b70a-4d0a-b52b-49c5bd4e4e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "<F I'=\u0017ata_i';estio'.p*=#rite=a=pro;ram=to=loa\u0017=t4e=same=\u0017ataset=-rom=t4e=Mo';oD =to=*our=s*stem=i'=\n",
    "DataFrame=-ormatF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d15452-168a-462c-be6b-3446f63a1b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "It appears you're asking for a Python program to load the same dataset from the data_ingestion.py file you\n",
    "mentioned earlier, and then convert it to a DataFrame format. If you want to load the dataset and convert it \n",
    "to a DataFrame using Pandas, you can follow this approach:\n",
    "\n",
    "Assuming you have a data_ingestion.py file that includes the load_dataset function, here's how you could write\n",
    "the code to achieve this:\n",
    "\n",
    "data_ingestion.py:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"Load and return the breast cancer dataset.\"\"\"\n",
    "    data = load_breast_cancer()\n",
    "    return data\n",
    "\n",
    "Your Jupyter Notebook:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "from data_ingestion import load_dataset\n",
    "\n",
    "# Load the dataset using the load_dataset function\n",
    "dataset = load_dataset()\n",
    "\n",
    "# Convert the dataset to a Pandas DataFrame\n",
    "data_df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "data_df['target'] = dataset.target\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(data_df.head())\n",
    "In this example, the load_dataset function from your data_ingestion.py file is imported into your Jupyter Notebook,\n",
    "and then the dataset is loaded and converted to a Pandas DataFrame. The DataFrame is created with the features \n",
    "from the dataset as columns, and an additional column 'target' is added to store the target labels.\n",
    "\n",
    "This code assumes that the load_dataset function from data_ingestion.py is in the same directory as your Jupyter\n",
    "Notebook. If it's in a different directory, you'll need to adjust the import path accordingly.\n",
    "\n",
    "Remember to replace these placeholder names with actual file and variable names that you're using in your project.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f89c2-24d4-4072-b00e-fdb9a0627b72",
   "metadata": {},
   "outputs": [],
   "source": [
    ")3F Do=t4e='ecessar*=-eature=e';i'eeri';=part=i'=\u0017ata_tra's-ormatio'.p*F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f70076-57b0-44f7-8699-b076114e3af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Certainly! If you have a data_transformation.py file and want to perform the necessary feature engineering \n",
    "steps on the loaded dataset,\n",
    "you can do the following:\n",
    "\n",
    "Assuming you have a data_transformation.py file that includes the necessary feature engineering functions,\n",
    "here's how you could write the code to perform the feature engineering in your Jupyter Notebook:\n",
    "\n",
    "data_transformation.py:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def perform_feature_engineering(data_df):\n",
    "    \"\"\"Perform feature engineering on the loaded DataFrame.\"\"\"\n",
    "    # Implement your feature engineering steps here\n",
    "    # For example, you can perform scaling, encoding, or other preprocessing\n",
    "    \n",
    "    # Return the modified DataFrame\n",
    "    return data_df\n",
    "Your Jupyter Notebook:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "from data_ingestion import load_dataset\n",
    "from data_transformation import perform_feature_engineering\n",
    "\n",
    "# Load the dataset using the load_dataset function\n",
    "dataset = load_dataset()\n",
    "\n",
    "# Convert the dataset to a Pandas DataFrame\n",
    "data_df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "data_df['target'] = dataset.target\n",
    "\n",
    "# Perform feature engineering using the perform_feature_engineering function\n",
    "data_df = perform_feature_engineering(data_df)\n",
    "\n",
    "# Display the first few rows of the modified DataFrame\n",
    "print(data_df.head())\n",
    "In this example, the perform_feature_engineering function from your data_transformation.py file is used to perform\n",
    "feature engineering steps on the DataFrame loaded from the dataset. The function takes the DataFrame as input and \n",
    "returns the modified DataFrame after performing feature engineering.\n",
    "\n",
    "As with the previous example, this assumes that your files are in the same directory as your Jupyter Notebook.\n",
    "If they are in different directories, make sure to adjust the import paths accordingly.\n",
    "\n",
    "Remember that the content and implementation of perform_feature_engineering will depend on the specific preprocessing \n",
    "steps you want to apply to your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9544b5-c1f4-4563-bf04-3b6fc20c4b77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9032701a-5002-4951-9fff-3d0b441d870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "))F Create=t4e=Mac4i'e=Lear'i';=mo\u0017el=i'=mo\u0017el_trai'er.p*F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d25cb-ad81-4a95-8394-f80ffe09b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Certainly! If you have a model_trainer.py file where you plan to define and train a machine learning model\n",
    "using the data you've prepared, you can structure your code like this:\n",
    "\n",
    "Assuming you have a model_trainer.py file with a function called train_model to define and train a machine \n",
    "learning model, here's how you could write the code to train the model in your Jupyter Notebook:\n",
    "\n",
    "model_trainer.py:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    \"\"\"Train a machine learning model.\"\"\"\n",
    "    # Initialize the model (for example, Logistic Regression)\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Train the model using the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "Your Jupyter Notebook:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "from data_ingestion import load_dataset\n",
    "from data_transformation import perform_feature_engineering\n",
    "from model_trainer import train_model\n",
    "\n",
    "# Load the dataset using the load_dataset function\n",
    "dataset = load_dataset()\n",
    "\n",
    "# Convert the dataset to a Pandas DataFrame\n",
    "data_df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "data_df['target'] = dataset.target\n",
    "\n",
    "# Perform feature engineering using the perform_feature_engineering function\n",
    "data_df = perform_feature_engineering(data_df)\n",
    "\n",
    "# Split the data into features (X) and target labels (y)\n",
    "X = data_df.drop('target', axis=1)\n",
    "y = data_df['target']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model using the train_model function\n",
    "trained_model = train_model(X_train, y_train)\n",
    "\n",
    "# Now you have a trained model that can be used for predictions\n",
    "In this example, the train_model function from your model_trainer.py file is used to define \n",
    "and train a machine learning model. The function takes the training data \n",
    "as input and returns the trained model. You can replace LogisticRegression with the machine \n",
    "learning algorithm of your choice.\n",
    "\n",
    "As before, make sure your files are in the correct directories or adjust the import paths accordingly.\n",
    "\n",
    "Remember that this is a basic example, and in practice, you would likely perform hyperparameter tuning and more advanced \n",
    "training steps to optimize your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b104a-39b5-4ef4-adc3-776c59491d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ea2681-158b-46ff-94b4-2bfc2de07f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\u001fF Use=Flask=to=\u0017eplo*=*our=project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96024a35-64cb-4711-ac71-726aab91e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Using Flask to deploy your machine learning project involves creating a web application that can serve predictions or provide a user interface for interacting with your model. Here's a basic outline of how you can set up a Flask application to deploy your project:\n",
    "\n",
    "Step 1: Install Flask\n",
    "If you haven't already, install Flask using the following command:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "pip install Flask\n",
    "Step 2: Create Flask App\n",
    "Create a new directory named app in your project's root directory. Inside the app directory, create a new file named app.py to define your Flask application.\n",
    "\n",
    "app/app.py:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from flask import Flask, request, jsonify\n",
    "import pandas as pd\n",
    "from data_ingestion import load_dataset\n",
    "from data_transformation import perform_feature_engineering\n",
    "from model_trainer import train_model\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the dataset using the load_dataset function\n",
    "dataset = load_dataset()\n",
    "\n",
    "# Convert the dataset to a Pandas DataFrame\n",
    "data_df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "data_df['target'] = dataset.target\n",
    "\n",
    "# Perform feature engineering using the perform_feature_engineering function\n",
    "data_df = perform_feature_engineering(data_df)\n",
    "\n",
    "# Split the data into features (X) and target labels (y)\n",
    "X = data_df.drop('target', axis=1)\n",
    "y = data_df['target']\n",
    "\n",
    "# Train the model using the train_model function\n",
    "trained_model = train_model(X, y)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return \"Welcome to the ML Project Deployment!\"\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        data = request.json['data']  # Assuming the input is a JSON with 'data' key\n",
    "        input_data = pd.DataFrame([data])\n",
    "        input_data = perform_feature_engineering(input_data)\n",
    "        prediction = trained_model.predict(input_data)\n",
    "        return jsonify({'prediction': prediction.tolist()})\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "Step 3: Run the Flask App\n",
    "Navigate to the app directory and run the Flask app:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "cd app\n",
    "python app.py\n",
    "Your Flask app should now be running locally at http://127.0.0.1:5000/. You can test it using a tool like curl, Postman, or by creating a simple HTML form to send POST requests.\n",
    "\n",
    "Please note that this is a very basic deployment setup. In a real-world scenario, you might want to handle more aspects like user authentication, error handling, optimizing model performance, and deploying to production servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c1ca6c-e7da-485f-b2b4-9255317916c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc97c715-3558-46ab-88ec-065ef767a920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e93e8-7524-49d2-b64c-bea38562ade6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd44c1db-ff65-4b8f-b8c9-515d80539ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
